{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/road_env.jpg\" style=\"zoom: 40%;\"/>\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and must reach the goal in cell $(8,6)$. The agent can move in the four directions (except when a wall is present), and for each step taken the agent receives a negative reward.\n",
    "In cells representing roads with intersections, the agent must wait for the traffic light to turn green before proceeding. At busy intersections (indicated by two traffic lights in the same cell), the agent will have to wait a long time to cross the intersection. This means that if the agent tries to move to another cell, the action may not succeed, causing the agent to remain in the same cell for an unknown amount of time.\n",
    "\n",
    "Assume that you do not have access to the motion model and to reward and that the problem is undiscounted, find a solution for the environment reported above with a suitable algorithm of your choice, motivating your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['W' 'Ts' 'R' 'R' 'R' 'R' 'Tl' 'R' 'R']\n",
      " ['W' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['R' 'Ts' 'R' 'Ts' 'R' 'R' 'Ts' 'W' 'W']\n",
      " ['W' 'W' 'W' 'R' 'W' 'W' 'R' 'Ts' 'R']\n",
      " ['W' 'R' 'R' 'Tl' 'W' 'W' 'W' 'R' 'W']\n",
      " ['W' 'R' 'W' 'R' 'Ts' 'R' 'R' 'Tl' 'R']\n",
      " ['W' 'R' 'W' 'W' 'R' 'W' 'W' 'R' 'W']\n",
      " ['R' 'Ts' 'R' 'R' 'Tl' 'R' 'G' 'Ts' 'R']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of cell (1, 6):  Tl\n",
      "Cell type of cell (1, 1):  Ts\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'RoadEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 15 # a very busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 10 # a less busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c6467600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon: #enters the if only if the random number is less than epsilon (i.e., with probability  epsilon)\n",
    "        return np.random.choice(q.shape[1]) #the size of first dimension of q is number of states, the second is number of actions \n",
    "    return q[state].argmax() #q[state] is a vector of q-values indexed by actions\n",
    "\n",
    "def softmax(q, state, temp):\n",
    "    \"\"\"\n",
    "    Softmax action selection function\n",
    "    \n",
    "    Args:\n",
    "    q: q table\n",
    "    state: agent's current state\n",
    "    temp: temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    e = np.exp(q[state] / temp) #a vector indexed by action where each value is e^(q(s,a)/T)\n",
    "    return np.random.choice(q.shape[1], p=e / e.sum()) # choose an action with probability e^(q(s,a)/T)/sum_{a'} e^(q(s,a')/T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5ab3b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, episodes= 500, alpha = 0.3, gamma = 0.9, expl_func = epsilon_greedy, expl_param = 0.1):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon or T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        state = environment.reset()\n",
    "        length = 0\n",
    "        tot_rew = 0\n",
    "        while True:\n",
    "            action = expl_func(q, state, expl_param)\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "            q[state, action] += alpha * (reward + gamma * q[next_state].max() - q[state, action])\n",
    "            state = next_state\n",
    "            length += 1\n",
    "            tot_rew += reward\n",
    "            if done:\n",
    "                break\n",
    "        rews[i] = tot_rew\n",
    "        lengths[i] = length\n",
    "        \n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8fb8337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENT THIS FUNCTION, YOU CAN CHANGE THE PARAMETERS FOR THE FUNCTION IF THIS IS USEFUL\n",
    "\n",
    "def sarsa(environment,expl_func=epsilon_greedy, episodes=500, alpha=.3, gamma=.9, expl_param=.1):\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        state = environment.reset()\n",
    "        length = 0\n",
    "        tot_rew = 0\n",
    "        action = expl_func(q, state, expl_param)\n",
    "        while True:\n",
    "            next_state, reward, done, _ = environment.step(action)\n",
    "            next_action = expl_func(q, next_state, expl_param)\n",
    "            q[state, action] += alpha * (reward + gamma * q[next_state, next_action] - q[state, action])\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            length += 1\n",
    "            tot_rew += reward\n",
    "            if done:\n",
    "                break\n",
    "        rews[i] = tot_rew\n",
    "        lengths[i] = length\n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "bed41066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def iter_val(environment, discount = 1, maxiters = 300, max_error = 1e-3): \n",
    "\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    #\n",
    "    U_1 = np.zeros(environment.observation_space.n)\n",
    "    for i in range(maxiters):\n",
    "        U = U_1.copy()\n",
    "        for s in range(environment.observation_space.n):\n",
    "            somm = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for s_1 in range(environment.observation_space.n):\n",
    "                    somm[a] += environment.T[s, a, s_1] * (environment.RS[s] + discount * U[s_1])\n",
    "            U_1[s] = np.max(somm)\n",
    "            if np.max(np.abs(U - U_1)) < max_error:\n",
    "                break\n",
    "    #\n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54421e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3aa046cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33635/1138467524.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution time: {round(timer() - t, 4)}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msolution_render\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iterative Value Iteration solution: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "policy, rews, lengths = iter_val(env)\n",
    "print(f\"Execution time: {round(timer() - t, 4)}s\") \n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.shape))\n",
    "print(\"Iterative Value Iteration solution: \")\n",
    "print(solution_render)\n",
    "\n",
    "\n",
    "window = 50  # Rolling window\n",
    "rewser = []\n",
    "lenser = []\n",
    "\n",
    "t = timer()\n",
    "_, rews, lengths = q_learning(env)\n",
    "#_, rews, lengths = q_learning(env, episodes, alpha, gamma, softmax, temp)\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"ls\": \"-\", \"label\": \"Q-Learning\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"ls\": \"-\", \"label\": \"Q-Learning\"})\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.shape))\n",
    "print(\"Iterative Value Iteration solution: \")\n",
    "print(solution_render)\n",
    "\n",
    "# SARSA\n",
    "t = timer()\n",
    "_, rews, lengths = sarsa(env)\n",
    "#_, rews, lengths = q_learning(env, episodes, alpha, gamma, softmax, temp)\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.shape))\n",
    "print(\"Iterative Value Iteration solution: \")\n",
    "print(solution_render)\n",
    "#_, rews, lengths = q_learning(env, episodes, alpha, gamma, softmax, temp)\n",
    "rews = rolling(rews, window)\n",
    "rewser.append({\"x\": np.arange(1, len(rews) + 1), \"y\": rews, \"label\": \"SARSA\"})\n",
    "lengths = rolling(lengths, window)\n",
    "lenser.append({\"x\": np.arange(1, len(lengths) + 1), \"y\": lengths, \"label\": \"SARSA\"})\n",
    "\n",
    "#print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "\n",
    "plot(rewser, \"Rewards\", \"Episodes\", \"Rewards\")\n",
    "plot(lenser, \"Lengths\", \"Episodes\", \"Lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65343ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
